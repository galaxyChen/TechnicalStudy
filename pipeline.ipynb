{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3490ac34-63ec-4e17-b1c6-e8c000b6c43c",
   "metadata": {},
   "source": [
    "## Pipeline简介\n",
    "在本文档中，我们将对Pipeline类的使用和实现进行简单的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd68a1-7df3-4114-ad22-edc472c2dcfa",
   "metadata": {},
   "source": [
    "### 简单调用\n",
    "首先我们从简单的调用开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e865cb-1399-41f9-acde-e66b5d2c236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/data/home/galaxycchen/code/libs/transformers/src/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': ' quel âge êtes-vous?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "print(en_fr_translator(\"How old are you?\")) #  quel âge êtes-vous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec01d2-ae50-42ab-bc94-2510b9c8426b",
   "metadata": {},
   "source": [
    "下载的文件有以下几个，其中主要的作用为：\n",
    "- config.json，用于配置模型\n",
    "- pytorch_model.bin，模型二进制文件\n",
    "- generation_config.json，生成策略配置\n",
    "- tokenizer_config.json，tokenizer配置\n",
    "- spiece.model，实际使用的分词模型\n",
    "- tokenizer.json，保存的tokenizer文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6920fe6-b235-4f76-99ba-c314bd253778",
   "metadata": {},
   "source": [
    "### Pipeline接口\n",
    "详见https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/pipelines#transformers.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfc941-19f2-4548-88bc-be29dfd2bc44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    task: str = None,\n",
    "    model: Optional = None,\n",
    "    config: Optional[Union[str, PretrainedConfig]] = None,\n",
    "    tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None,\n",
    "    feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None,\n",
    "    image_processor: Optional[Union[str, BaseImageProcessor]] = None,\n",
    "    framework: Optional[str] = None,\n",
    "    revision: Optional[str] = None,\n",
    "    use_fast: bool = True,\n",
    "    use_auth_token: Optional[Union[str, bool]] = None,\n",
    "    device: Optional[Union[int, str, \"torch.device\"]] = None,\n",
    "    device_map=None,\n",
    "    torch_dtype=None,\n",
    "    trust_remote_code: Optional[bool] = None,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    pipeline_class: Optional[Any] = None,\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Utility factory method to build a [`Pipeline`].\n",
    "\n",
    "    Pipelines are made of:\n",
    "\n",
    "        - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n",
    "        - A [model](model) to make predictions from the inputs.\n",
    "        - Some (optional) post processing for enhancing model's output.\n",
    "\n",
    "    Args:\n",
    "        task (`str`):\n",
    "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
    "\n",
    "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
    "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
    "            - `\"conversational\"`: will return a [`ConversationalPipeline`].\n",
    "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
    "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
    "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
    "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
    "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
    "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
    "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
    "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
    "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
    "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
    "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
    "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
    "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
    "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
    "              [`TextClassificationPipeline`].\n",
    "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
    "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
    "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
    "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
    "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
    "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
    "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
    "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
    "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
    "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
    "\n",
    "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
    "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
    "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
    "            [`TFPreTrainedModel`] (for TensorFlow).\n",
    "\n",
    "            If not provided, the default for the `task` will be loaded.\n",
    "        config (`str` or [`PretrainedConfig`], *optional*):\n",
    "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
    "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
    "\n",
    "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
    "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
    "            `task`'s default model's config is used instead.\n",
    "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
    "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
    "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
    "\n",
    "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
    "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
    "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
    "            will be loaded.\n",
    "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
    "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
    "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
    "\n",
    "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
    "            models. Multi-modal models will also require a tokenizer to be passed.\n",
    "\n",
    "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
    "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
    "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
    "            for the given `task` will be loaded.\n",
    "        framework (`str`, *optional*):\n",
    "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
    "            installed.\n",
    "\n",
    "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
    "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
    "            provided.\n",
    "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
    "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
    "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
    "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
    "        use_fast (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
    "        use_auth_token (`str` or *bool*, *optional*):\n",
    "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
    "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
    "        device (`int` or `str` or `torch.device`):\n",
    "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
    "            pipeline will be allocated.\n",
    "        device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
    "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
    "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
    "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
    "            for more information).\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        torch_dtype (`str` or `torch.dtype`, *optional*):\n",
    "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
    "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
    "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
    "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
    "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
    "        model_kwargs:\n",
    "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
    "            **model_kwargs)` function.\n",
    "        kwargs:\n",
    "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
    "            corresponding pipeline class for possible values).\n",
    "\n",
    "    Returns:\n",
    "        [`Pipeline`]: A suitable pipeline for the task.\n",
    "    ```\"\"\"\n",
    "    \n",
    "    # code for loading model, pipeline_class, and other kwargs\n",
    "    if tokenizer is not None:\n",
    "        kwargs[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    if feature_extractor is not None:\n",
    "        kwargs[\"feature_extractor\"] = feature_extractor\n",
    "\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    if image_processor is not None:\n",
    "        kwargs[\"image_processor\"] = image_processor\n",
    "\n",
    "    if device is not None:\n",
    "        kwargs[\"device\"] = device\n",
    "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de974b-d26f-4bcc-9206-43095c974e87",
   "metadata": {},
   "source": [
    "上述代码最后返回的是针对输入参数推断的任务类，在翻译的场景中，即TranslationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db93e2-fd42-41d5-94fb-d40afa879cb2",
   "metadata": {},
   "source": [
    "### 自定义Pipeline\n",
    "在实际研究翻译Pipeline的实现之前，我们可以先来看看怎么自定义一个Pipeline，这将有助于我们理解Pipeline的通用接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b0655-fbbb-47cc-babb-3956a54dbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "\n",
    "class MyPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        _sanitize_parameters exists to allow users to pass any parameters whenever they wish, \n",
    "        be it at initialization time pipeline(...., maybe_arg=4) or at call time pipe = pipeline(...); output = pipe(...., maybe_arg=4).\n",
    "        The returns of _sanitize_parameters are the 3 dicts of kwargs that will be passed directly to \n",
    "        preprocess, _forward, and postprocess. \n",
    "        Don’t fill anything if the caller didn’t call with any extra parameter. \n",
    "        That allows to keep the default arguments in the function definition which is always more “natural”.\n",
    "        \"\"\"\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, maybe_arg=2):\n",
    "        \"\"\"\n",
    "        preprocess will take the originally defined inputs, and turn them into something feedable to the model. \n",
    "        It might contain more information and is usually a Dict.\n",
    "        \"\"\"\n",
    "        model_input = Tensor(inputs[\"input_ids\"])\n",
    "        return {\"model_input\": model_input}\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        \"\"\"\n",
    "        _forward is the implementation detail and is not meant to be called directly. \n",
    "        forward is the preferred called method as it contains safeguards to make sure everything is working on the expected device. \n",
    "        If anything is linked to a real model it belongs in the _forward method, anything else is in the preprocess/postprocess.\n",
    "        \"\"\"\n",
    "        # model_inputs == {\"model_input\": model_input}\n",
    "        outputs = self.model(**model_inputs)\n",
    "        # Maybe {\"logits\": Tensor(...)}\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        \"\"\"\n",
    "        postprocess methods will take the output of _forward and turn it into the final output that was decided earlier.\n",
    "        \"\"\"\n",
    "        best_class = model_outputs[\"logits\"].softmax(-1)\n",
    "        return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210d951-5c69-4f63-834e-58f4fa8a7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个自定义参数的例子\n",
    "pipe = pipeline(\"my-new-task\")\n",
    "pipe(\"This is a test\")\n",
    "# [{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}, {\"label\": \"3-star\", \"score\": 0.05},\n",
    "# {\"label\": \"4-star\", \"score\": 0.025}, {\"label\": \"5-star\", \"score\": 0.025}]\n",
    "pipe(\"This is a test\", top_k=2)\n",
    "# [{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4996f-1f81-400b-a3e6-28def1fb965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对应的接口函数需要修改如下\n",
    "def postprocess(self, model_outputs, top_k=5):\n",
    "    best_class = model_outputs[\"logits\"].softmax(-1)\n",
    "    # Add logic to handle top_k\n",
    "    return best_class\n",
    "\n",
    "\n",
    "def _sanitize_parameters(self, **kwargs):\n",
    "    preprocess_kwargs = {}\n",
    "    if \"maybe_arg\" in kwargs:\n",
    "        preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "\n",
    "    postprocess_kwargs = {}\n",
    "    if \"top_k\" in kwargs:\n",
    "        postprocess_kwargs[\"top_k\"] = kwargs[\"top_k\"]\n",
    "    return preprocess_kwargs, {}, postprocess_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a18539-58d4-4139-856b-88e456413a8e",
   "metadata": {},
   "source": [
    "#### 注册新的Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fada28-c587-4e6e-8bd6-cba88facaa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"new-task\",\n",
    "    pipeline_class=MyPipeline,\n",
    "    pt_model=AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# 或者添加一个默认的模型\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"new-task\",\n",
    "    pipeline_class=MyPipeline,\n",
    "    pt_model=AutoModelForSequenceClassification,\n",
    "    default={\"pt\": (\"user/awesome_model\", \"abcdef\")},\n",
    "    type=\"text\",  # current support type: text, audio, image, multimodal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b7d35-0bee-44d2-90d2-49c53091a6c0",
   "metadata": {},
   "source": [
    "### TranslationPipeline解析\n",
    "了解了如何进行自定义Pipeline之后，我们就可以去研究翻译的Pipeline是如何实现的了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03889c-d454-4e57-b3b7-4e9cf6317b7e",
   "metadata": {},
   "source": [
    "#### 注册部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c2277-1e80-4150-9cce-291c3c45e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认提供三种不同翻译模型，根据输入参数的不同来加载不同的默认模型\n",
    "{\n",
    "    \"translation\": {\n",
    "        \"impl\": TranslationPipeline,\n",
    "        \"tf\": (TFAutoModelForSeq2SeqLM,) if is_tf_available() else (),\n",
    "        \"pt\": (AutoModelForSeq2SeqLM,) if is_torch_available() else (),\n",
    "        \"default\": {\n",
    "            (\"en\", \"fr\"): {\"model\": {\"pt\": (\"t5-base\", \"686f1db\"), \"tf\": (\"t5-base\", \"686f1db\")}},\n",
    "            (\"en\", \"de\"): {\"model\": {\"pt\": (\"t5-base\", \"686f1db\"), \"tf\": (\"t5-base\", \"686f1db\")}},\n",
    "            (\"en\", \"ro\"): {\"model\": {\"pt\": (\"t5-base\", \"686f1db\"), \"tf\": (\"t5-base\", \"686f1db\")}},\n",
    "        },\n",
    "        \"type\": \"text\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f1272-193f-4b34-9a07-7c0841853519",
   "metadata": {},
   "source": [
    "#### TranslationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8a38c-a362-4882-9b72-7f6a67dac3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPipeline(Text2TextGenerationPipeline):\n",
    "    \"\"\"\n",
    "    Translates from one language to another.\n",
    "\n",
    "    This translation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
    "    `\"translation_xx_to_yy\"`.\n",
    "\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a translation task. See the\n",
    "    up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=translation).\n",
    "    For a list of available parameters, see the [following\n",
    "    documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "    en_fr_translator(\"How old are you?\")\n",
    "    ```\"\"\"\n",
    "\n",
    "    # Used in the return key of the pipeline.\n",
    "    return_name = \"translation\"\n",
    "\n",
    "    def check_inputs(self, input_length: int, min_length: int, max_length: int):\n",
    "        if input_length > 0.9 * max_length:\n",
    "            logger.warning(\n",
    "                f\"Your input_length: {input_length} is bigger than 0.9 * max_length: {max_length}. You might consider \"\n",
    "                \"increasing your max_length manually, e.g. translator('...', max_length=400)\"\n",
    "            )\n",
    "        return True\n",
    "\n",
    "    def preprocess(self, *args, truncation=TruncationStrategy.DO_NOT_TRUNCATE, src_lang=None, tgt_lang=None):\n",
    "        \"\"\"\n",
    "        def _build_translation_inputs(\n",
    "            self, raw_inputs, return_tensors: str, src_lang: Optional[str], tgt_lang: Optional[str], **extra_kwargs\n",
    "        ):\n",
    "            #Used by translation pipeline, to prepare inputs for the generate function\n",
    "            if src_lang is None or tgt_lang is None:\n",
    "                raise ValueError(\"Translation requires a `src_lang` and a `tgt_lang` for this model\")\n",
    "            self.src_lang = src_lang\n",
    "            inputs = self(raw_inputs, add_special_tokens=True, return_tensors=return_tensors, **extra_kwargs)\n",
    "            tgt_lang_id = self.convert_tokens_to_ids(tgt_lang)\n",
    "            inputs[\"forced_bos_token_id\"] = tgt_lang_id\n",
    "        return inputs\n",
    "        \"\"\"\n",
    "        if getattr(self.tokenizer, \"_build_translation_inputs\", None):\n",
    "            return self.tokenizer._build_translation_inputs(\n",
    "                *args, return_tensors=self.framework, truncation=truncation, src_lang=src_lang, tgt_lang=tgt_lang\n",
    "            )\n",
    "        else:\n",
    "            return super()._parse_and_tokenize(*args, truncation=truncation)\n",
    "\n",
    "    def _sanitize_parameters(self, src_lang=None, tgt_lang=None, **kwargs):\n",
    "        preprocess_params, forward_params, postprocess_params = super()._sanitize_parameters(**kwargs)\n",
    "        if src_lang is not None:\n",
    "            preprocess_params[\"src_lang\"] = src_lang\n",
    "        if tgt_lang is not None:\n",
    "            preprocess_params[\"tgt_lang\"] = tgt_lang\n",
    "        if src_lang is None and tgt_lang is None:\n",
    "            # Backward compatibility, direct arguments use is preferred.\n",
    "            task = kwargs.get(\"task\", self.task)\n",
    "            items = task.split(\"_\")\n",
    "            if task and len(items) == 4:\n",
    "                # translation, XX, to YY\n",
    "                preprocess_params[\"src_lang\"] = items[1]\n",
    "                preprocess_params[\"tgt_lang\"] = items[3]\n",
    "        return preprocess_params, forward_params, postprocess_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f0f1e-0cb0-4980-97f4-ff03b9294703",
   "metadata": {},
   "source": [
    "实际上，这个类的代码非常简洁，只修改了极个别的函数，我们进一步深入到父类中去研究实现\n",
    "#### Text2TextGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcda1f-7ed6-4ee8-acb2-8682108ea273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2TextGenerationPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Pipeline for text to text generation using seq2seq models.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import pipeline\n",
    "\n",
    "    >>> generator = pipeline(model=\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "    >>> generator(\n",
    "    ...     \"answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "    ... )\n",
    "    [{'generated_text': 'question: Who created the RuPERTa-base?'}]\n",
    "    ```\n",
    "\n",
    "    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
    "\n",
    "\n",
    "    This Text2TextGenerationPipeline pipeline can currently be loaded from [`pipeline`] using the following task\n",
    "    identifier: `\"text2text-generation\"`.\n",
    "\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a translation task. See the\n",
    "    up-to-date list of available models on\n",
    "    [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation). For a list of available\n",
    "    parameters, see the [following\n",
    "    documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    text2text_generator = pipeline(\"text2text-generation\")\n",
    "    text2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")\n",
    "    ```\"\"\"\n",
    "\n",
    "    # Used in the return key of the pipeline.\n",
    "    return_name = \"generated\"\n",
    "\n",
    "    def _sanitize_parameters(\n",
    "        self,\n",
    "        return_tensors=None,\n",
    "        return_text=None,\n",
    "        return_type=None,\n",
    "        clean_up_tokenization_spaces=None,\n",
    "        truncation=None,\n",
    "        stop_sequence=None,\n",
    "        **generate_kwargs,\n",
    "    ):\n",
    "        preprocess_params = {}\n",
    "        if truncation is not None:\n",
    "            preprocess_params[\"truncation\"] = truncation\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "\n",
    "        postprocess_params = {}\n",
    "        if return_tensors is not None and return_type is None:\n",
    "            return_type = ReturnType.TENSORS if return_tensors else ReturnType.TEXT\n",
    "        if return_type is not None:\n",
    "            postprocess_params[\"return_type\"] = return_type\n",
    "\n",
    "        if clean_up_tokenization_spaces is not None:\n",
    "            postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n",
    "\n",
    "        if stop_sequence is not None:\n",
    "            stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
    "            if len(stop_sequence_ids) > 1:\n",
    "                warnings.warn(\n",
    "                    \"Stopping on a multiple token sequence is not yet supported on transformers. The first token of\"\n",
    "                    \" the stop sequence will be used as the stop sequence string in the interim.\"\n",
    "                )\n",
    "            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        r\"\"\"\n",
    "        Generate the output text(s) using text(s) given as inputs.\n",
    "\n",
    "        Args:\n",
    "            args (`str` or `List[str]`):\n",
    "                Input text for the encoder.\n",
    "            return_tensors (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to include the tensors of predictions (as token indices) in the outputs.\n",
    "            return_text (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to include the decoded texts in the outputs.\n",
    "            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to clean up the potential extra spaces in the text output.\n",
    "            truncation (`TruncationStrategy`, *optional*, defaults to `TruncationStrategy.DO_NOT_TRUNCATE`):\n",
    "                The truncation strategy for the tokenization within the pipeline. `TruncationStrategy.DO_NOT_TRUNCATE`\n",
    "                (default) will never truncate, but it is sometimes desirable to truncate the input to fit the model's\n",
    "                max_length instead of throwing an error down the line.\n",
    "            generate_kwargs:\n",
    "                Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
    "                corresponding to your framework [here](./model#generative-models)).\n",
    "\n",
    "        Return:\n",
    "            A list or a list of list of `dict`: Each result comes as a dictionary with the following keys:\n",
    "\n",
    "            - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
    "            - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
    "              ids of the generated text.\n",
    "        \"\"\"\n",
    "\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        if (\n",
    "            isinstance(args[0], list)\n",
    "            and all(isinstance(el, str) for el in args[0])\n",
    "            and all(len(res) == 1 for res in result)\n",
    "        ):\n",
    "            return [res[0] for res in result]\n",
    "        return result\n",
    "\n",
    "    def _parse_and_tokenize(self, *args, truncation):\n",
    "        prefix = self.model.config.prefix if self.model.config.prefix is not None else \"\"\n",
    "        if isinstance(args[0], list):\n",
    "            if self.tokenizer.pad_token_id is None:\n",
    "                raise ValueError(\"Please make sure that the tokenizer has a pad_token_id when using a batch input\")\n",
    "            args = ([prefix + arg for arg in args[0]],)\n",
    "            padding = True\n",
    "\n",
    "        elif isinstance(args[0], str):\n",
    "            args = (prefix + args[0],)\n",
    "            padding = False\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\" `args[0]`: {args[0]} have the wrong format. The should be either of type `str` or type `list`\"\n",
    "            )\n",
    "        inputs = self.tokenizer(*args, padding=padding, truncation=truncation, return_tensors=self.framework)\n",
    "        # This is produced by tokenizers but is an invalid generate kwargs\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            del inputs[\"token_type_ids\"]\n",
    "        return inputs\n",
    "        \n",
    "    def preprocess(self, inputs, truncation=TruncationStrategy.DO_NOT_TRUNCATE, **kwargs):\n",
    "        inputs = self._parse_and_tokenize(inputs, truncation=truncation, **kwargs)\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        if self.framework == \"pt\":\n",
    "            in_b, input_length = model_inputs[\"input_ids\"].shape\n",
    "        elif self.framework == \"tf\":\n",
    "            in_b, input_length = tf.shape(model_inputs[\"input_ids\"]).numpy()\n",
    "\n",
    "        generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", self.model.config.min_length)\n",
    "        generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\", self.model.config.max_length)\n",
    "        self.check_inputs(input_length, generate_kwargs[\"min_length\"], generate_kwargs[\"max_length\"])\n",
    "        output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
    "        out_b = output_ids.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            output_ids = output_ids.reshape(in_b, out_b // in_b, *output_ids.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            output_ids = tf.reshape(output_ids, (in_b, out_b // in_b, *output_ids.shape[1:]))\n",
    "        return {\"output_ids\": output_ids}\n",
    "\n",
    "    def postprocess(self, model_outputs, return_type=ReturnType.TEXT, clean_up_tokenization_spaces=False):\n",
    "        records = []\n",
    "        for output_ids in model_outputs[\"output_ids\"][0]:\n",
    "            if return_type == ReturnType.TENSORS:\n",
    "                record = {f\"{self.return_name}_token_ids\": output_ids}\n",
    "            elif return_type == ReturnType.TEXT:\n",
    "                record = {\n",
    "                    f\"{self.return_name}_text\": self.tokenizer.decode(\n",
    "                        output_ids,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "                    )\n",
    "                }\n",
    "            records.append(record)\n",
    "        return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea3e6e-88f8-421f-a806-d32d4b2f657a",
   "metadata": {},
   "source": [
    "### 实例：将nllb模型的推理修改成Pipeline形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dcd2c-1b71-4d32-849f-e1581b480fc5",
   "metadata": {},
   "source": [
    "#### 原始推理形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49842c04-4a0a-477d-91b6-dfee1c6db347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "text=\"你好,世界!\"\n",
    "# text=\"hello world!\"\n",
    "src_lang=\"zho_Hans\"\n",
    "tgt_lang=\"eng_Latn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\", use_auth_token=True, src_lang=src_lang\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", use_auth_token=True)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang], max_length=30\n",
    ")\n",
    "print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04daeb-3f7e-4684-870e-252cd430b9fd",
   "metadata": {},
   "source": [
    "需要特别定制的地方：\n",
    "- tokenizer\n",
    "- model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2695e84c-e861-4205-a634-50e947629e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hello, world!'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法一：实现初始化模型和tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\", use_auth_token=True, src_lang=src_lang\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", use_auth_token=True)\n",
    "\n",
    "nllb = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "nllb(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e539e546-6b69-499d-bef8-8250918bf404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hello, world!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法二：只传入设置，让pipeline自行加载\n",
    "model_name=\"facebook/nllb-200-distilled-600M\"\n",
    "nllb = pipeline(\"translation\", model=model_name, tokenizer=(model_name, {\"src_lang\": src_lang}), src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "nllb(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fc96e-9775-47ff-b211-e1d2a53cb72a",
   "metadata": {},
   "source": [
    "注意在方法二中，我们在model参数只传入了一个字符串，tokenizer中传入了一个tuple，第二个参数是一个dict，用来描述tokenizer初始化时的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84cb60-9808-418c-9312-a22b6083cff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
